[convex function] [strictly convex] [concave] [strictly concave]

# Prerequisite Knowledge

# Explanation of Notations

- $\mathbb{R}$: real number set
- $\mathbb{R}_{+}$: nonnegative real number set
- $\mathbb{R}_{++}$: positive real number set
- $\mathbf{S}_{++}$: positive definite matrix
- $\langle a,b\rangle$: the (dot) product of $a$ and $b$. $a$ and $b$ are usually vectors.
- $\nabla f$: the gradient of function $f$

# Definition

## Convex function

First of all, let's have a look at the definition of a *convex function* $f:\mathbb{R}^n\rightarrow\mathbb{R}$. If

- the domain of $f$, i.e. $\boldsymbol{\mathrm{dom}}f$, is a convex set,
- a parameter $\theta\in\mathbb{R}$ with $0\leq\theta\leq1$
- for any two points in the domain $x,y\in\textbf{dom}f$, we have

$$f(\theta x+(1-\theta)y)\leq\theta f(x)+(1-\theta)f(y),$$

such a function $f$ is a convex function. The inequality above is sometimes called *Jensen's inequality*. For the right-hand side of the inequality, since $\theta$ is in $[0,1]$, it is the line segment enclosed by $x$ and $y$. The inequality illustrates that the line segment between any two points in the function should be in or beyond the function itself.

Since $x$ and $y$ are two arbitrary points (can be same) in the function, another expression to determine the convexity of a function is "*a function is convex if and only if it is convex when restricted to any line that intersects its domain*".

### Extend the definition

As the extension of the convex set, the definition of the convexity of a function can be extended from using two points to using more points. If $f$ is convex, and $x_1,\cdots,x_k\in\textbf{dom}f$, and $\theta_1,\cdots,\theta_k\leq0$ with $\theta_1+\cdots+\theta_k=1$, then

$$f(\theta_1x_1+\cdots+\theta_kx_k)\leq\theta)1f(x_1)+\cdots+\theta_kf(x_k).$$

This definition by finite sums can be extended to infinite sums. Assume a probability density function $\theta(x)$ defined in the domain set $S\subseteq\textbf{dom}f$, we have $\int_S\theta(x)dx=1$. If the integrals exist, then

$$f\left(\int_S\theta(x)xdx\right)\leq\int_Sf(x)\theta(x)dx.$$

Note that we can take any probability measure $\theta(x)$ as long as it is a subset of the domain of $f$. In the point of view from probability theory, the above inequality is equivalent to

$$f(\mathbb{E}[x])\leq\mathbb{E}\left[f(x)\right].$$

## Strictly convex function

If the strict equality holds everywhere except for the two end points $x$ and $y$, i.e. whenever $x\neq y$ and $0<\theta<1$. This can also be expressed geometrically, that everywhere in the function is not flat (is convex).

## Concave function and strictly concave function

If a function $f$ is convex, then $-f$ is concave. The definition of strictly concave is similar to that of strictly convex.

# Some examples of convex functions

- All *linear* ($f(x)=Ax$) and *affine* ($f(x)=Ax+b$) functions are convex.
- *Exponential*. $e^{ax}$ is convex on $\mathbb{R}$, for any $a\in\mathbb{R}$.
- *Powers*. $x^a$ is convex on $\mathbb{R}_{++}$ when $a\geq1$ or $a\leq0$, and concave for $0<a<1$.
- *Powers of absolute value*. $|x|^p$ is convex on $\mathbb{R}$ for $p\geq1$.
- *Logarithm*. $\log x$ is concave on $\mathbb{R}_{++}$.
- *Negative entropy*. $x\log x$ (either on $\mathbb{R}_{++}$ or on $\mathbb{R}_{+}$, defined as 0 for $x=0$) is convex.
- *Norms*. Every norm is convex on $\mathbb{R}^n$. (see Minkowski Inequality)
- *Max function*. $f(x)=\max(x_1,\cdots,x_n)$ is convex on $\mathbb{R}^n$.
- *Quadratic-over-linear function*. $f(x,y)=x^2/y$ is convex in $\textbf{dom}f=\mathbb{R}\times\mathbb{R}_{++}=\left\{(x,y)\in\mathbb{R^2|y>0}\right\}$.
- *Log-sum-exp*. 
- *Log-determinant*. $f(X)=\log\det X$ is concave on $\boldsymbol{\mathrm{dom}}f=\mathbf{S}_{++}^{n}$.

# More on definition of convex functions (not on the text book)

A function $f$ is convex is equivalent to the followings:

1. from the standpoint of the gradient or subgradient

    $$\begin{aligned}
        \text{gradient:}\quad&f(y)\geq f(x)+\langle\nabla f(x),y-x\rangle,\quad\forall y\\
        \text{subgradient:}\quad&f(y)\geq f(x)+\langle g,y-x\rangle,\quad\forall y,
    \end{aligned}$$

    where $g$ is the subgradient of function $f$, and $x$ and $y$ are the points on the function. The subgradient is used when $f$ is not differentiable.

2. from the standpoint of the Hessian matrix

    $$\nabla^2f(x)\geq 0.$$

## Proof

### Proof of 1

If $f$ is convex, the function $f$ is monotone (increasing).

> A monotonic function is a nondecreasing and nonincreasing function. Therefore, the derivative (gradient) of this function is nonnegative or nonpositive cosrrespondingly.

A convex function has a monotonically increasing gradient, if differentiable. A monotonically increasing function has also another definition with respect to derivative, i.e.

$$\langle\nabla f(x)-\nabla f(y),x-y\rangle\geq0.$$

For two points $x$ and $y$, assume it is true that $f(y)\geq f(x)+\langle\nabla,y-x\rangle$, then we have also $f(x)\geq f(x)+\langle\nabla,x-y\rangle$. Sum up these two inequalities, we have

$$f(x)+f(y)\geq f(x)+f(y)+\langle\nabla f(x),y-x\rangle+\langle\nabla f(y),x-y\rangle.$$

Therefore, the first theorem is proved. $\square$

# Extension to Complex-Valued Variables

The principle of the optimization problem is that the objective function return real value, since only real-valued function can be ordered, and hence can be compared, i.e. what is the minimum point and what is the maximum point.

## Some Examples of Real-Valued Objective Function with Complex-Valued Variables

- $f(x)=x^Hx$ for $x\in\mathbb{C}^n$ is convex.
  - Proof: