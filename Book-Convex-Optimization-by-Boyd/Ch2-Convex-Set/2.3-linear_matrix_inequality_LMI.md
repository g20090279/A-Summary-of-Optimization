- Keywords: linear matrix inequality, positive semidefinite matrix,
- Last Update: Mar. 03, 2022

---

# Review of positive semidefinite matrix

First of all, a *positive semidefinite matrix* $A$ must be *symmetric*, i.e. $A=A^T$, and square. Without loss of generality, we assume that $A$ is of size $n$-by-$n$. There are two equivalent definitions of a *positive semidefinite* (PSD) matrix A:
- eigenvalues are nonnegative,
- for all $z\in\mathbb{R}^n$, we have $z^TAz\geq0$.

A matrix $A$ being a PSD is denoted by $A\succeq0$. The notation $\mathbb{S}^{n}\subseteq\mathbb{R}^{n\times n}$ is used to denote the field of symmetric square matrix of size $n$-by-$n$. $\mathbb{S}^n_{+}\subseteq\mathbb{S}^n$ denotes the set of PSD.

# Definition of linear matrix inequality

The *linear matrix inequality* (LMI) is a constraint of the form

$$F(x)=F_0+\sum_{i=1}^{m}x_iF_i\succeq0,$$

where $F_0,\cdots,F_m\in\mathbb{S}^n$, i.e., $n$-by-$n$ *symmetric* matrices. $x=[x_1,\cdots,x_m]^T\in\mathbb{R}^{m}$.

The *most important* application of LMI is that it is the standard form of the inequality constraint in *semidefinite programming* (SDP), which has a standard form as

$$\begin{aligned}
   \min\quad&c^Tx\\
   \text{s.t.}\quad&x_1F_1+x_2F_2+\cdots+x_nF_n+G\preceq0\\
   &Ax=b.
\end{aligned}$$

# Remarks

- Note that $F(x)$ is PSD. In other words, an *affine* function $F(\cdot)$ maps a finite-dimensional vector space $\mathbb{R}^{m}$ to the PSD space $\mathbb{S}_{+}^{n}$.
- Since $\mathbb{S}_{+}^n$ is convex, the LMI $F(\cdot)$ defines a convex constrains on $x$.
- LMI has no analytical solution in general.

# Examples: formulating inequality constraints in LMI standard form

## Example 1

The matrix $\begin{bmatrix}1&x\\x&1\end{bmatrix}$ can be decomposed into

$$\begin{bmatrix}1&x\\x&1\end{bmatrix}=\begin{bmatrix}1&0\\0&1\end{bmatrix}+x\begin{bmatrix}0&1\\1&0\end{bmatrix}.$$

The right-hand side is the linear combination of two symmetric matrix.

## Example 2

Write the constraints $y>0,y-x^2>0$ into LMI standard form. First of all, we rewrite the constraints into a matrix with the Syvester's criterion as

$$\begin{cases}
   y>0\\
   y-x^2>0 
\end{cases}\Leftrightarrow\begin{bmatrix}
   y & x \\ x & 1 
\end{bmatrix}\succeq0.$$

After that, we can expand the matrix in in a standard form of LMI

$$\begin{bmatrix}y & x \\ x & 1\end{bmatrix}=\begin{bmatrix}0 & 0 \\ 0 & 1\end{bmatrix}+y\begin{bmatrix}1 & 0 \\ 0 & 0\end{bmatrix}+x\begin{bmatrix}0 & 1\\ 1 & 0\end{bmatrix}\succeq0$$

## Example 3

We can write the inequality constraint $x_1^2+x_2^2<1$ as

$$\begin{bmatrix}
   1 & 0 & x_1\\
   0 & 1 & x_2\\
   x_1 & x_2 & 1 
\end{bmatrix}.$$

We can check the leading minors:

$$\begin{aligned}
   1&>0\\
   \begin{bmatrix}
   1&0\\0&1
   \end{bmatrix}&>0\\
   1\begin{bmatrix}
     1&x_2\\x_2&1 
   \end{bmatrix}-0\begin{bmatrix}
        1&x_1\\x_1&1 
   \end{bmatrix}+x_1\begin{bmatrix}
      0&1\\x_1&x_2 
   \end{bmatrix}&>0,
\end{aligned}$$

where the last inequality is equivalent to the original inequality constraint.

# Examples of application

## Problem 1: eigenvalue minimization

Assume $A(x):=A_0+A_1x_1+\cdots+A_nx_n$ is a LMI, where $A_i\in\mathbb{S}^n,i=1,2,\cdots,n$, and $x=[x_1,x_2,\cdots,x_n]^T$. Find the optimal $x$ that minimizes the largest eigenvalue of $A(x)$, i.e.

$$p=\min_{x}\lambda_{\max}A(x),$$

where $\lambda_{\max}(\cdot)$ returns the largest eigenvalue of the matrix. This optimization problem is unconstrained.

The $\lambda_{\max}(\cdot)$ function is not a linear function, and not convex. We can transform the objective function by using an auxiliary variable $t$ to

$$\lambda_{\max}A(x)\leq t\Leftrightarrow A(x)-tI\preceq0.$$

Note that $A\preceq0$ means that matrix $A$ is negative semidefinite. The original optimization problem is then transformed to

$$\begin{aligned}
    \min_{x,t}\quad&t\\
    \text{s.t.}\quad&A(x)-tI\preceq0,
\end{aligned}$$

which is a SDP.

